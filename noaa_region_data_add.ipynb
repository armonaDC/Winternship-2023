{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my regions are specified by the Califonria Fire Science Consortium https://www.cafiresci.org/northern-california\n",
    "\n",
    "metadata on noaa stations https://www.ncei.noaa.gov/access/homr/reports/mshr\n",
    "\n",
    "noaa dataset api use specified https://www.ncei.noaa.gov/support/access-data-service-api-user-documentation\n",
    "\n",
    "explanation of dataset lables https://www.ncei.noaa.gov/pub/data/metadata/documents/GSOYReadme.txt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This function is meant to take a fire location and year from our dataset and use them to establish \n",
    "- the region fire was in \n",
    "- the stations in the region \n",
    "- the stations within a distance of fire \n",
    "- the climate data for those stations since 1950\n",
    "- returns the average data between stations in a selected year\n",
    "\n",
    "this uses NOAA metadata on station locations to identify stations in area and request those stations from the NOAA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from shapely.geometry import Point, Polygon\n",
    "import re\n",
    "\n",
    "#Fire regions established the california fire science consortium \n",
    "skipped = 0\n",
    "region_a = Polygon([(40.467845490773,-121.22314453125),(36.08906040282,-115.7080078125),(34.836349990764,-118.5205078125),(38.980762765016,-121.88232421875)])\n",
    "region_b = Polygon([(38.632350249008,-121.71203613281),(37.508013519657,-123.42590332031),(33.062115144155,-120.13000488281),(32.06212626392,-116.24084472656),(32.80393039071,-115.95520019531),(34.432317516745,-116.61437988281),(34.739838698614,-118.13049316406),(34.757892712137,-118.56994628906)])\n",
    "region_c = Polygon([(36.012015045348,-115.47180175781),(34.794192467556,-118.43811035156),(34.39626834484,-116.15295410156),(32.080955638786,-115.66955566406),(32.619260868167,-111.62658691406),(36.047554116355,-115.18615722656)])\n",
    "region_d = Polygon([(38.692817813434,-121.71752929688),(37.621647607458,-123.36547851562),(39.511264410098,-124.90356445312),(42.251715998571,-124.94750976562),(42.316738909445,-119.36645507812),(38.949602728462,-119.25659179688),(40.604379386497,-121.25610351562),(38.881217307773,-122.02514648438)])\n",
    "regions = [region_a,region_b, region_c, region_d]\n",
    "\n",
    "\n",
    "\n",
    "#establish all stations within each region by checking for coordinates inside each region \n",
    "stations_a = []\n",
    "stations_b = []\n",
    "stations_c = []\n",
    "stations_d = []\n",
    "\n",
    "file = open('./emshr_lite.txt', 'r')\n",
    "count = 0\n",
    "while True:\n",
    "  line = file.readline()\n",
    "  if not line: #check for end of file \n",
    "    break\n",
    "  \n",
    "  count += 1\n",
    "  if count < 3: #insure that only the lines with data are scraped (only past the third index)\n",
    "    continue\n",
    "  \n",
    "  station_id = line[74:85]\n",
    "  station_coordinate_lat = line[272:281].strip()\n",
    "  station_coordinate_lon = line[282:292].strip()\n",
    "\n",
    "  \n",
    "  if station_coordinate_lat == '' or station_coordinate_lon == '' or station_id.strip() == '':\n",
    "    skipped += 1\n",
    "    print(skipped)\n",
    "    continue\n",
    "  station_coordinate_lat = float(station_coordinate_lat)\n",
    "  station_coordinate_lon = float(station_coordinate_lon)\n",
    "  \n",
    "  station_coordinate = (station_coordinate_lat, station_coordinate_lon)\n",
    "  station_coordinate = Point(station_coordinate)\n",
    "  \n",
    "  for regional,stational in [(region_a, stations_a), (region_b, stations_b), (region_c, stations_c), (region_d, stations_d)]:\n",
    "    if regional.contains(station_coordinate):  \n",
    "      stational.append([station_id, station_coordinate])\n",
    "  \n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9276\\1286301329.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mregional\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstational\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregion_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstations_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mregion_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstations_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mregion_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstations_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mregion_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstations_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mregional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstation_coordinate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m       \u001b[0mstational\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstation_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstation_coordinate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Juan\\anaconda3\\lib\\site-packages\\shapely\\geometry\\base.py\u001b[0m in \u001b[0;36mcontains\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;34m\"\"\"Returns True if the geometry contains the other, else False\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_maybe_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapely\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcontains_properly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Juan\\anaconda3\\lib\\site-packages\\shapely\\decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marray_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_flag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_flags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Juan\\anaconda3\\lib\\site-packages\\shapely\\predicates.py\u001b[0m in \u001b[0;36mcontains\u001b[1;34m(a, b, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \"\"\"\n\u001b[1;32m--> 540\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from shapely.geometry import Point, Polygon\n",
    "import re\n",
    "\n",
    "#Fire regions established the california fire science consortium \n",
    "\n",
    "region_a = Polygon([(40.467845490773,-121.22314453125),(36.08906040282,-115.7080078125),(34.836349990764,-118.5205078125),(38.980762765016,-121.88232421875)])\n",
    "region_b = Polygon([(38.632350249008,-121.71203613281),(37.508013519657,-123.42590332031),(33.062115144155,-120.13000488281),(32.06212626392,-116.24084472656),(32.80393039071,-115.95520019531),(34.432317516745,-116.61437988281),(34.739838698614,-118.13049316406),(34.757892712137,-118.56994628906)])\n",
    "region_c = Polygon([(36.012015045348,-115.47180175781),(34.794192467556,-118.43811035156),(34.39626834484,-116.15295410156),(32.080955638786,-115.66955566406),(32.619260868167,-111.62658691406),(36.047554116355,-115.18615722656)])\n",
    "region_d = Polygon([(38.692817813434,-121.71752929688),(37.621647607458,-123.36547851562),(39.511264410098,-124.90356445312),(42.251715998571,-124.94750976562),(42.316738909445,-119.36645507812),(38.949602728462,-119.25659179688),(40.604379386497,-121.25610351562),(38.881217307773,-122.02514648438)])\n",
    "regions = [region_a,region_b, region_c, region_d]\n",
    "\n",
    "\n",
    "\n",
    "#establish all stations within each region by checking for coordinates inside each region \n",
    "stations_a = []\n",
    "stations_b = []\n",
    "stations_c = []\n",
    "stations_d = []\n",
    "\n",
    "file = open('./emshr_lite.txt', 'r')\n",
    "count = 0\n",
    "while True:\n",
    "  line = file.readline()\n",
    "  if not line: #check for end of file \n",
    "    break\n",
    "  \n",
    "  count += 1\n",
    "  if count < 3: #insure that only the lines with data are scraped (only past the third index)\n",
    "    continue\n",
    "  \n",
    "  station_id = line[74:85]\n",
    "  station_coordinate_lat = line[272:281].strip()\n",
    "  station_coordinate_lon = line[282:292].strip()\n",
    "\n",
    "  if station_coordinate_lat == '' or station_coordinate_lon == '' or station_id.strip() == '':\n",
    "    continue\n",
    "  station_coordinate_lat = float(station_coordinate_lat)\n",
    "  station_coordinate_lon = float(station_coordinate_lon)\n",
    "  \n",
    "  station_coordinate = (station_coordinate_lat, station_coordinate_lon)\n",
    "  station_coordinate = Point(station_coordinate)\n",
    "  \n",
    "  for regional,stational in [(region_a, stations_a), (region_b, stations_b), (region_c, stations_c), (region_d, stations_d)]:\n",
    "    if regional.contains(station_coordinate):  \n",
    "      stational.append([station_id, station_coordinate])\n",
    "  \n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "def lat_lon_to_shapely(lat,lon): #this allows us to use basic lat and lon to create a shapely object for initial function\n",
    "  return Point(lat,lon)\n",
    "\n",
    "#takes the fire location and returns region of fire \n",
    "def find_region(fire_location):\n",
    "  for region in regions:\n",
    "    if region.contains(fire_location.centroid):\n",
    "      return region\n",
    "    # else:\n",
    "    #   return None\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#takes the region and returns a list of stations and their coordinates from metadata \n",
    "def region_station_list(region): \n",
    "  for regions, stations in [(region_a, stations_a),(region_b, stations_b),(region_c, stations_c),(region_d, stations_d)]:\n",
    "    if regions == region:\n",
    "      return stations\n",
    " \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "#takes the list of stations and fire location to return stations within a degree of distance from fire\n",
    "def closest_to_fire_center(station_list, fire_location):\n",
    "  fire_center = fire_location.centroid\n",
    "  closest_station_list = []\n",
    "  for station in station_list:\n",
    "    if station[1].distance(fire_center) < 1:\n",
    "      closest_station_list.append((station[0], station[1].distance(fire_center)))\n",
    "  closest_station_list.sort(key=lambda closest_station_list: closest_station_list[1])\n",
    "  closest_station_list = closest_station_list[:50]\n",
    "  return [x[0] for x in closest_station_list]\n",
    "\n",
    "\n",
    "\n",
    "#takes a string of closest stations and requests their data from noaa api\n",
    "def request_station_data(closest_stations):\n",
    "  closest_stations = ','.join(closest_stations)\n",
    "  \n",
    "  header = {'token': 'SXCObXMuhoovZxrVZpurQqmsHdvJWyUJ'}\n",
    "  r=requests.get(f'https://www.ncei.noaa.gov/access/services/data/v1?dataset=global-summary-of-the-year&stations={closest_stations}&&startDate=1950-01-01T00:00:00z&&endDate=2022-01-01&format=csv&bbox=-124.40959,32.534156,-114.131211,42.009518', headers=header)\n",
    "  if r.status_code != 200:\n",
    "    print(r.status_code)\n",
    "    print(r.text)\n",
    "  df = pd.read_csv(io.StringIO(r.text))\n",
    "  return df\n",
    "\n",
    "\n",
    "#takes the station data returns a dataframe row of the averaged values for year of fire \n",
    "#add a recurrance here to request more than 50 please \n",
    "def filter_station_data(station_df, fire_year):\n",
    "\n",
    "  station_df = station_df[['DATE','DP01','DP10','DP1X','DSNW','DX32','DX70','DX90','EMSN','EMXP','EMXT','PRCP', 'SNOW','TMAX',]]\n",
    "  \n",
    "  station_df = station_df.rename(columns={\"DP01\": \"PDepth>0.01in\", \"DP10\": \"PDepth>0.1in\", \"DP1X\": \"PDepth>1in\", \"DSNW\": \"SDepth>1in\", \"DX32\": \"DaysT<32d\", \"DX70\": \"DaysT>70d\", \"DX90\": \"DaysT>90d\",\n",
    "  \"EMSN\": \"HighestDaySnow\", \"EMXP\": \"HighestDayRain\", \"EMXT\": \"HighestDayTemp\", \"PRCP\": \"TotalRain\", \"SNOW\": \"TotalSnow\", \"TMAX\": \"AvgMonthMax\"})\n",
    " \n",
    "  \n",
    "  station_df = station_df[station_df['DATE'] == fire_year]\n",
    "  station_df = station_df.rename(columns = {'DATE': 'Fire_Year'})\n",
    "  #the method of filling missing values should be changed for final library touches \n",
    "  station_df = pd.DataFrame(station_df.mean()).T\n",
    "  return station_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "event_climate_data was originally meant to be the function to use on dataset but I added convert_event_data() to handle latitude and longitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_climate_data(fire_location, fire_year): #function that returns the average local climate data for year of the fire as a row to add to dataset \n",
    "  region = find_region(fire_location)\n",
    "  if region == None:\n",
    "    return None\n",
    "  stations = region_station_list(region)\n",
    "  closest_stations = closest_to_fire_center(stations, fire_location)\n",
    "  stations_df = request_station_data(closest_stations)\n",
    "  filtered_df = filter_station_data(stations_df,fire_year)\n",
    "  return filtered_df\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate applicaton of this is to input a fire's lat, lon and year to create a row of the average climate data from the 50 closest stations within a degree of distance from the fire for the year \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_event_data(lat,lon,fire_year): #this allows for direct conversion from what we have in our fire dataset \n",
    "    fire_location = lat_lon_to_shapely(lat, lon)\n",
    "    return event_climate_data(fire_location, fire_year)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "piece by piece function tests:\n",
    "- if test location is region center it should be in that region \n",
    "- region_station_list should return list of IDs and coordinates \n",
    "- closest to fire center should be less than total stations and only be IDs\n",
    "- request station data should return a a full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions: #region function is accurate \n",
    "    test_location  = region.centroid\n",
    "    if find_region(test_location) != region:\n",
    "        print(f\"test failed: {test_location} should have been {region}\")\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "region = region_a\n",
    "test_stations = region_station_list(region_a) #this returns a list of stations and coordinates \n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "test_closest = closest_to_fire_center(test_stations, region_a.centroid)\n",
    "print(test_closest)\n",
    "print(str(len(test_stations)) + ' in region vs ' + str(len(test_closest)) + ' closest') #closest should contain less than test stations\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "test_df = request_station_data(test_closest) #returns dataframe of data for stations since 1950\n",
    "test_df\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "filter_station_data(test_df, 2001) #returns a row with the average data for the year "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the convert_event_data() function on each row in our fire outcome dataset\n",
    "\n",
    "convert_event_data(latitude, longitude, year)\n",
    "\n",
    "- latitude is latitude \n",
    "- longitude is latitude \n",
    "- year is the year of the event \n",
    "- returns a row of the average climate data of the closest 50 noaa stations to be added onto event datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df = pd.read_csv('/home/devin/winternship2023/datasets/Cal1.2.csv')\n",
    "# fire_df[\"Fire_Year\"] = fire_df[\"Fire_Year\"].apply(lambda x : pd.to_datetime(x).year)\n",
    "# display(fire_df.head())\n",
    "#add on the column names we going to add to fire dataset \n",
    "new_columns = ['PDepth>0.01in','PDepth>0.1in','PDepth>1in','SDepth>1in','DaysT<32d','DaysT>70d','DaysT>90d','HighestDaySnow','HighestDayRain','HighestDayTemp','TotalRain','TotalSnow','AvgMonthMax']\n",
    "old_columns = ['Fire_Name','Lat','Long','Fire_Year','Fire_Month','Total_area_burned(ha)','Unchanged(ha)','Low_Severity(ha)','Moderate_Severity(ha)','High_Severity(ha)','FuelTreatment','TSLF(months)']\n",
    "all_columns = old_columns + new_columns\n",
    "fire_df[new_columns] = ''\n",
    "fire_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our process on a row of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "new_frame = pd.DataFrame(columns = all_columns)\n",
    "\n",
    "og_row = fire_df.iloc[[index]]\n",
    "display(og_row)\n",
    "\n",
    "add_on_row = convert_event_data(fire_df['Lat'][index], fire_df['Long'][index], fire_df['Fire_Year'][index])[new_columns]\n",
    "display(add_on_row) \n",
    "\n",
    "og_row.loc[index, new_columns] = add_on_row.loc[0, new_columns]\n",
    "display(og_row)\n",
    "\n",
    "new_frame.loc[index] = og_row.loc[index]\n",
    "display(new_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_frame = pd.DataFrame(columns = all_columns) #where we will stack the new rows \n",
    "for index in range(len(fire_df)):\n",
    "  if find_region(lat_lon_to_shapely(fire_df['Lat'][index], fire_df['Long'][index])) == None: #if the fire is outside of our regions then we skip it\n",
    "    continue\n",
    "\n",
    "  og_row = fire_df.iloc[[index]]\n",
    "  add_on_row = convert_event_data(fire_df['Lat'][index], fire_df['Long'][index], fire_df['Fire_Year'][index])[new_columns]\n",
    "  \n",
    "  og_row.loc[index, new_columns] = add_on_row.loc[0, new_columns]\n",
    "  \n",
    "  new_frame.loc[index] = og_row.loc[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_columns = [ 'Lat', 'Long', 'Fire_Year','Fire_Month', 'Total_area_burned(ha)','Moderate_Severity(ha)','High_Severity(ha)','PDepth>0.01in','PDepth>0.1in','PDepth>1in','SDepth>1in','DaysT<32d','DaysT>70d','DaysT>90d','HighestDaySnow','HighestDayRain','HighestDayTemp','TotalRain','TotalSnow','AvgMonthMax']\n",
    "\n",
    "\n",
    "#replace this method of filling values please god future me I worked on documentation \n",
    "for column in num_columns:\n",
    "  new_frame[column].fillna(value= new_frame[column].mean(), inplace=True)\n",
    "\n",
    "new_frame.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_frame.to_csv('../datasets/fire_climate_set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/fire_climate_set.csv')\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f506ea6ad3cdd143e00922cd7413cd7a1b83580fe7abf8de8fb11d06ac5448c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
